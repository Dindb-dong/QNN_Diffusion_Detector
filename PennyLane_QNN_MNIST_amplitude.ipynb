{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e3fc07",
   "metadata": {},
   "source": [
    "# PennyLane Quantum Neural Network (QNN) for MNIST Binary Classification\n",
    "\n",
    "This notebook implements a QNN using PennyLane to classify handwritten digits 0 and 1 from the MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c9c162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pennylane as qml\n",
    "\n",
    "# Device and parameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "size = 16\n",
    "n_qubits = 8\n",
    "n_layers = 4\n",
    "batch_size = 16\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26ed12fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "def load_dataset(name=\"MNIST\"):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "    ])\n",
    "    if name == \"MNIST\":\n",
    "        train = datasets.MNIST(root=\"MNIST\", train=True, download=True, transform=transform)\n",
    "        test = datasets.MNIST(root=\"MNIST\", train=False, download=True, transform=transform)\n",
    "    return train, test\n",
    "\n",
    "def generate_subset(dataset):\n",
    "    return [(x, torch.tensor(y, dtype=torch.float32)) for x, y in dataset if y in [0, 1]]\n",
    "\n",
    "train_raw, test_raw = load_dataset(\"MNIST\")\n",
    "train_dataset = generate_subset(train_raw)\n",
    "test_dataset = generate_subset(test_raw)\n",
    "\n",
    "# PennyLane device\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55069713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum circuit definition\n",
    "def layer(weights):\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(weights[i], wires=i)\n",
    "    for i in range(n_qubits - 1):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "\n",
    "def qnode_fn(inputs, weights):\n",
    "    qml.templates.AmplitudeEmbedding(inputs, wires=range(n_qubits), normalize=True)\n",
    "    for l in range(n_layers):\n",
    "        layer(weights[l])\n",
    "    return qml.expval(qml.PauliZ(n_qubits - 1))\n",
    "\n",
    "qnode = qml.QNode(qnode_fn, dev, interface=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8891189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch wrapper\n",
    "class TorchQNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(0.01 * torch.randn(n_layers, n_qubits))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, size * size)\n",
    "        x = x / torch.norm(x, dim=1, keepdim=True)  # normalize\n",
    "        outputs = [ qnode(x[i], self.weights) for i in range(x.shape[0]) ]\n",
    "        return torch.stack(outputs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58f00499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate(model, dataset, name):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    loss_val = 0\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y = y.to(device).to(torch.float32)\n",
    "            pred = model(X)\n",
    "            loss_val += loss_fn(pred, y).item()\n",
    "            pred_label = torch.stack([1 - pred, pred], dim=1).argmax(1)\n",
    "            correct += (pred_label == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    print(f\"{name} Accuracy: {correct/total:.3f}, Loss: {loss_val/len(loader):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af4c6b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_dataset, test_dataset):\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        for batch, (X, y) in enumerate(loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            X = X.to(torch.float32)\n",
    "            y = y.to(device).float()\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Batch {batch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        evaluate(model, train_dataset, 'Train')\n",
    "        evaluate(model, test_dataset, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd72b3",
   "metadata": {},
   "source": [
    "## 학습 시작\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d961c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 0.6927\n",
      "Epoch 1, Batch 20, Loss: 0.6908\n",
      "Epoch 1, Batch 40, Loss: 0.6891\n",
      "Epoch 1, Batch 60, Loss: 0.6872\n",
      "Epoch 1, Batch 80, Loss: 0.6866\n",
      "Epoch 1, Batch 100, Loss: 0.6839\n",
      "Epoch 1, Batch 120, Loss: 0.6824\n",
      "Epoch 1, Batch 140, Loss: 0.6723\n",
      "Epoch 1, Batch 160, Loss: 0.6639\n",
      "Epoch 1, Batch 180, Loss: 0.6531\n",
      "Epoch 1, Batch 200, Loss: 0.6496\n",
      "Epoch 1, Batch 220, Loss: 0.6437\n",
      "Epoch 1, Batch 240, Loss: 0.6487\n",
      "Epoch 1, Batch 260, Loss: 0.6269\n",
      "Epoch 1, Batch 280, Loss: 0.6129\n",
      "Epoch 1, Batch 300, Loss: 0.6267\n",
      "Epoch 1, Batch 320, Loss: 0.5938\n",
      "Epoch 1, Batch 340, Loss: 0.6166\n",
      "Epoch 1, Batch 360, Loss: 0.5968\n",
      "Epoch 1, Batch 380, Loss: 0.5948\n",
      "Epoch 1, Batch 400, Loss: 0.5777\n",
      "Epoch 1, Batch 420, Loss: 0.5979\n",
      "Epoch 1, Batch 440, Loss: 0.5652\n",
      "Epoch 1, Batch 460, Loss: 0.5568\n",
      "Epoch 1, Batch 480, Loss: 0.5946\n",
      "Epoch 1, Batch 500, Loss: 0.5969\n",
      "Epoch 1, Batch 520, Loss: 0.5916\n",
      "Epoch 1, Batch 540, Loss: 0.5799\n",
      "Epoch 1, Batch 560, Loss: 0.6056\n",
      "Epoch 1, Batch 580, Loss: 0.5676\n",
      "Epoch 1, Batch 600, Loss: 0.5568\n",
      "Epoch 1, Batch 620, Loss: 0.5382\n",
      "Epoch 1, Batch 640, Loss: 0.6254\n",
      "Epoch 1, Batch 660, Loss: 0.5864\n",
      "Epoch 1, Batch 680, Loss: 0.5868\n",
      "Epoch 1, Batch 700, Loss: 0.5502\n",
      "Epoch 1, Batch 720, Loss: 0.5918\n",
      "Epoch 1, Batch 740, Loss: 0.5797\n",
      "Epoch 1, Batch 760, Loss: 0.5835\n",
      "Epoch 1, Batch 780, Loss: 0.5313\n",
      "Train Accuracy: 0.520, Loss: 0.564\n",
      "Test Accuracy: 0.526, Loss: 0.561\n",
      "Epoch 2, Batch 0, Loss: 0.5944\n",
      "Epoch 2, Batch 20, Loss: 0.5752\n",
      "Epoch 2, Batch 40, Loss: 0.5705\n",
      "Epoch 2, Batch 60, Loss: 0.5856\n",
      "Epoch 2, Batch 80, Loss: 0.5576\n",
      "Epoch 2, Batch 100, Loss: 0.5864\n",
      "Epoch 2, Batch 120, Loss: 0.5672\n",
      "Epoch 2, Batch 140, Loss: 0.5326\n",
      "Epoch 2, Batch 160, Loss: 0.5525\n",
      "Epoch 2, Batch 180, Loss: 0.5535\n",
      "Epoch 2, Batch 200, Loss: 0.5816\n",
      "Epoch 2, Batch 220, Loss: 0.5543\n",
      "Epoch 2, Batch 240, Loss: 0.5383\n",
      "Epoch 2, Batch 260, Loss: 0.5432\n",
      "Epoch 2, Batch 280, Loss: 0.5710\n",
      "Epoch 2, Batch 300, Loss: 0.5393\n",
      "Epoch 2, Batch 320, Loss: 0.5925\n",
      "Epoch 2, Batch 340, Loss: 0.5348\n",
      "Epoch 2, Batch 360, Loss: 0.5477\n",
      "Epoch 2, Batch 380, Loss: 0.5245\n",
      "Epoch 2, Batch 400, Loss: 0.5957\n",
      "Epoch 2, Batch 420, Loss: 0.5397\n",
      "Epoch 2, Batch 440, Loss: 0.5676\n",
      "Epoch 2, Batch 460, Loss: 0.5212\n",
      "Epoch 2, Batch 480, Loss: 0.5392\n",
      "Epoch 2, Batch 500, Loss: 0.5413\n",
      "Epoch 2, Batch 520, Loss: 0.4950\n",
      "Epoch 2, Batch 540, Loss: 0.5201\n",
      "Epoch 2, Batch 560, Loss: 0.5192\n",
      "Epoch 2, Batch 580, Loss: 0.4876\n",
      "Epoch 2, Batch 600, Loss: 0.5364\n",
      "Epoch 2, Batch 620, Loss: 0.5262\n",
      "Epoch 2, Batch 640, Loss: 0.5305\n",
      "Epoch 2, Batch 660, Loss: 0.5516\n",
      "Epoch 2, Batch 680, Loss: 0.5460\n",
      "Epoch 2, Batch 700, Loss: 0.5486\n",
      "Epoch 2, Batch 720, Loss: 0.5659\n",
      "Epoch 2, Batch 740, Loss: 0.5386\n",
      "Epoch 2, Batch 760, Loss: 0.5800\n",
      "Epoch 2, Batch 780, Loss: 0.5410\n",
      "Train Accuracy: 0.794, Loss: 0.545\n",
      "Test Accuracy: 0.813, Loss: 0.541\n",
      "Epoch 3, Batch 0, Loss: 0.5395\n",
      "Epoch 3, Batch 20, Loss: 0.5538\n",
      "Epoch 3, Batch 40, Loss: 0.5612\n",
      "Epoch 3, Batch 60, Loss: 0.5357\n",
      "Epoch 3, Batch 80, Loss: 0.5453\n",
      "Epoch 3, Batch 100, Loss: 0.5261\n",
      "Epoch 3, Batch 120, Loss: 0.5062\n",
      "Epoch 3, Batch 140, Loss: 0.5307\n",
      "Epoch 3, Batch 160, Loss: 0.5144\n",
      "Epoch 3, Batch 180, Loss: 0.5547\n",
      "Epoch 3, Batch 200, Loss: 0.5704\n",
      "Epoch 3, Batch 220, Loss: 0.5402\n",
      "Epoch 3, Batch 240, Loss: 0.5360\n",
      "Epoch 3, Batch 260, Loss: 0.5266\n",
      "Epoch 3, Batch 280, Loss: 0.5553\n",
      "Epoch 3, Batch 300, Loss: 0.5467\n",
      "Epoch 3, Batch 320, Loss: 0.5285\n",
      "Epoch 3, Batch 340, Loss: 0.5464\n",
      "Epoch 3, Batch 360, Loss: 0.5316\n",
      "Epoch 3, Batch 380, Loss: 0.5706\n",
      "Epoch 3, Batch 400, Loss: 0.5332\n",
      "Epoch 3, Batch 420, Loss: 0.5382\n",
      "Epoch 3, Batch 440, Loss: 0.5409\n",
      "Epoch 3, Batch 460, Loss: 0.5369\n",
      "Epoch 3, Batch 480, Loss: 0.5297\n",
      "Epoch 3, Batch 500, Loss: 0.5339\n",
      "Epoch 3, Batch 520, Loss: 0.5253\n",
      "Epoch 3, Batch 540, Loss: 0.5288\n",
      "Epoch 3, Batch 560, Loss: 0.5299\n",
      "Epoch 3, Batch 580, Loss: 0.5148\n",
      "Epoch 3, Batch 600, Loss: 0.5655\n",
      "Epoch 3, Batch 620, Loss: 0.5033\n",
      "Epoch 3, Batch 640, Loss: 0.5022\n",
      "Epoch 3, Batch 660, Loss: 0.6034\n",
      "Epoch 3, Batch 680, Loss: 0.5371\n",
      "Epoch 3, Batch 700, Loss: 0.5747\n",
      "Epoch 3, Batch 720, Loss: 0.5301\n",
      "Epoch 3, Batch 740, Loss: 0.5412\n",
      "Epoch 3, Batch 760, Loss: 0.5271\n",
      "Epoch 3, Batch 780, Loss: 0.5329\n",
      "Train Accuracy: 0.756, Loss: 0.535\n",
      "Test Accuracy: 0.776, Loss: 0.531\n",
      "Epoch 4, Batch 0, Loss: 0.5395\n",
      "Epoch 4, Batch 20, Loss: 0.5180\n",
      "Epoch 4, Batch 40, Loss: 0.5535\n",
      "Epoch 4, Batch 60, Loss: 0.5079\n",
      "Epoch 4, Batch 80, Loss: 0.5169\n",
      "Epoch 4, Batch 100, Loss: 0.5519\n",
      "Epoch 4, Batch 120, Loss: 0.5559\n",
      "Epoch 4, Batch 140, Loss: 0.5158\n",
      "Epoch 4, Batch 160, Loss: 0.5333\n",
      "Epoch 4, Batch 180, Loss: 0.5128\n",
      "Epoch 4, Batch 200, Loss: 0.5386\n",
      "Epoch 4, Batch 220, Loss: 0.5316\n",
      "Epoch 4, Batch 240, Loss: 0.5169\n",
      "Epoch 4, Batch 260, Loss: 0.4922\n",
      "Epoch 4, Batch 280, Loss: 0.5183\n",
      "Epoch 4, Batch 300, Loss: 0.5180\n",
      "Epoch 4, Batch 320, Loss: 0.5279\n",
      "Epoch 4, Batch 340, Loss: 0.5350\n",
      "Epoch 4, Batch 360, Loss: 0.5482\n",
      "Epoch 4, Batch 380, Loss: 0.5128\n",
      "Epoch 4, Batch 400, Loss: 0.5083\n",
      "Epoch 4, Batch 420, Loss: 0.5680\n",
      "Epoch 4, Batch 440, Loss: 0.5501\n",
      "Epoch 4, Batch 460, Loss: 0.5568\n",
      "Epoch 4, Batch 480, Loss: 0.5226\n",
      "Epoch 4, Batch 500, Loss: 0.5165\n",
      "Epoch 4, Batch 520, Loss: 0.4866\n",
      "Epoch 4, Batch 540, Loss: 0.4956\n",
      "Epoch 4, Batch 560, Loss: 0.5118\n",
      "Epoch 4, Batch 580, Loss: 0.5275\n",
      "Epoch 4, Batch 600, Loss: 0.5475\n",
      "Epoch 4, Batch 620, Loss: 0.5310\n",
      "Epoch 4, Batch 640, Loss: 0.5160\n",
      "Epoch 4, Batch 660, Loss: 0.5269\n",
      "Epoch 4, Batch 680, Loss: 0.5188\n",
      "Epoch 4, Batch 700, Loss: 0.5555\n",
      "Epoch 4, Batch 720, Loss: 0.6023\n",
      "Epoch 4, Batch 740, Loss: 0.5546\n",
      "Epoch 4, Batch 760, Loss: 0.5268\n",
      "Epoch 4, Batch 780, Loss: 0.5533\n",
      "Train Accuracy: 0.879, Loss: 0.529\n",
      "Test Accuracy: 0.889, Loss: 0.526\n",
      "Epoch 5, Batch 0, Loss: 0.5554\n",
      "Epoch 5, Batch 20, Loss: 0.5122\n",
      "Epoch 5, Batch 40, Loss: 0.5591\n",
      "Epoch 5, Batch 60, Loss: 0.5132\n",
      "Epoch 5, Batch 80, Loss: 0.5058\n",
      "Epoch 5, Batch 100, Loss: 0.4947\n",
      "Epoch 5, Batch 120, Loss: 0.5281\n",
      "Epoch 5, Batch 140, Loss: 0.4869\n",
      "Epoch 5, Batch 160, Loss: 0.5784\n",
      "Epoch 5, Batch 180, Loss: 0.5015\n",
      "Epoch 5, Batch 200, Loss: 0.5623\n",
      "Epoch 5, Batch 220, Loss: 0.5632\n",
      "Epoch 5, Batch 240, Loss: 0.5768\n",
      "Epoch 5, Batch 260, Loss: 0.5663\n",
      "Epoch 5, Batch 280, Loss: 0.5032\n",
      "Epoch 5, Batch 300, Loss: 0.4847\n",
      "Epoch 5, Batch 320, Loss: 0.5285\n",
      "Epoch 5, Batch 340, Loss: 0.5101\n",
      "Epoch 5, Batch 360, Loss: 0.5372\n",
      "Epoch 5, Batch 380, Loss: 0.5199\n",
      "Epoch 5, Batch 400, Loss: 0.5086\n",
      "Epoch 5, Batch 420, Loss: 0.5440\n",
      "Epoch 5, Batch 440, Loss: 0.5176\n",
      "Epoch 5, Batch 460, Loss: 0.5691\n",
      "Epoch 5, Batch 480, Loss: 0.5356\n",
      "Epoch 5, Batch 500, Loss: 0.5218\n",
      "Epoch 5, Batch 520, Loss: 0.5345\n",
      "Epoch 5, Batch 540, Loss: 0.5036\n",
      "Epoch 5, Batch 560, Loss: 0.5089\n",
      "Epoch 5, Batch 580, Loss: 0.5244\n",
      "Epoch 5, Batch 600, Loss: 0.5388\n",
      "Epoch 5, Batch 620, Loss: 0.5068\n",
      "Epoch 5, Batch 640, Loss: 0.5271\n",
      "Epoch 5, Batch 660, Loss: 0.5105\n",
      "Epoch 5, Batch 680, Loss: 0.5043\n",
      "Epoch 5, Batch 700, Loss: 0.5414\n",
      "Epoch 5, Batch 720, Loss: 0.5449\n",
      "Epoch 5, Batch 740, Loss: 0.5422\n",
      "Epoch 5, Batch 760, Loss: 0.4949\n",
      "Epoch 5, Batch 780, Loss: 0.5445\n",
      "Train Accuracy: 0.930, Loss: 0.528\n",
      "Test Accuracy: 0.935, Loss: 0.525\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run\n",
    "model = TorchQNN().to(device)\n",
    "train(model, train_dataset, test_dataset)\n",
    "\n",
    "# Predict single sample\n",
    "model.eval()\n",
    "import random\n",
    "idx = random.randint(0, len(test_dataset) - 1)\n",
    "x = test_dataset[idx][0]\n",
    "y_true = test_dataset[idx][1].item()\n",
    "\n",
    "plt.imshow(x[0], cmap='gray')\n",
    "plt.title(f\"True label: {int(y_true)}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(x.view(1, 1, size, size)).cpu().numpy()[0]\n",
    "    prob = [1 - out, out]\n",
    "    pred = np.argmax(prob)\n",
    "    print(f'Predicted label: {pred}')\n",
    "    plt.stem([0, 1], prob)\n",
    "    plt.xticks([0, 1])\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('QNN Prediction')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34606ef6",
   "metadata": {},
   "source": [
    "## 모델 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0efa816b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved to: torchqnn_mnist.pth\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장\n",
    "model_path = \"torchqnn_mnist.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"✅ Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15c902",
   "metadata": {},
   "source": [
    "## 모델 로딩, 평가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a643a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 모델 객체 생성\n",
    "model = TorchQNN().to(device)\n",
    "\n",
    "# 저장된 파라미터 불러오기\n",
    "model.load_state_dict(torch.load(\"torchqnn_mnist.pth\"))\n",
    "model.eval()  # 평가 모드로 전환\n",
    "\n",
    "print(\"✅ Model loaded and ready for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e381957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 0.5319\n",
      "Epoch 1, Batch 20, Loss: 0.5592\n",
      "Epoch 1, Batch 40, Loss: 0.5410\n",
      "Epoch 1, Batch 60, Loss: 0.5118\n",
      "Epoch 1, Batch 80, Loss: 0.5580\n",
      "Epoch 1, Batch 100, Loss: 0.5443\n",
      "Epoch 1, Batch 120, Loss: 0.5447\n",
      "Epoch 1, Batch 140, Loss: 0.5244\n",
      "Epoch 1, Batch 160, Loss: 0.5301\n",
      "Epoch 1, Batch 180, Loss: 0.5246\n",
      "Epoch 1, Batch 200, Loss: 0.5256\n",
      "Epoch 1, Batch 220, Loss: 0.5084\n",
      "Epoch 1, Batch 240, Loss: 0.5732\n",
      "Epoch 1, Batch 260, Loss: 0.5439\n",
      "Epoch 1, Batch 280, Loss: 0.5365\n",
      "Epoch 1, Batch 300, Loss: 0.5549\n",
      "Epoch 1, Batch 320, Loss: 0.5338\n",
      "Epoch 1, Batch 340, Loss: 0.5199\n",
      "Epoch 1, Batch 360, Loss: 0.5094\n",
      "Epoch 1, Batch 380, Loss: 0.5478\n",
      "Epoch 1, Batch 400, Loss: 0.5495\n",
      "Epoch 1, Batch 420, Loss: 0.5126\n",
      "Epoch 1, Batch 440, Loss: 0.5318\n",
      "Epoch 1, Batch 460, Loss: 0.5322\n",
      "Epoch 1, Batch 480, Loss: 0.5363\n",
      "Epoch 1, Batch 500, Loss: 0.5164\n",
      "Epoch 1, Batch 520, Loss: 0.5277\n",
      "Epoch 1, Batch 540, Loss: 0.5173\n",
      "Epoch 1, Batch 560, Loss: 0.5101\n",
      "Epoch 1, Batch 580, Loss: 0.5558\n",
      "Epoch 1, Batch 600, Loss: 0.5438\n",
      "Epoch 1, Batch 620, Loss: 0.5160\n",
      "Epoch 1, Batch 640, Loss: 0.5403\n",
      "Epoch 1, Batch 660, Loss: 0.5159\n",
      "Epoch 1, Batch 680, Loss: 0.5631\n",
      "Epoch 1, Batch 700, Loss: 0.5048\n",
      "Epoch 1, Batch 720, Loss: 0.5618\n",
      "Epoch 1, Batch 740, Loss: 0.5424\n",
      "Epoch 1, Batch 760, Loss: 0.5431\n",
      "Epoch 1, Batch 780, Loss: 0.5216\n",
      "Train Accuracy: 0.659, Loss: 0.532\n",
      "Test Accuracy: 0.682, Loss: 0.527\n",
      "Epoch 2, Batch 0, Loss: 0.5145\n",
      "Epoch 2, Batch 20, Loss: 0.5223\n",
      "Epoch 2, Batch 40, Loss: 0.5105\n",
      "Epoch 2, Batch 60, Loss: 0.5079\n",
      "Epoch 2, Batch 80, Loss: 0.5078\n",
      "Epoch 2, Batch 100, Loss: 0.5431\n",
      "Epoch 2, Batch 120, Loss: 0.5291\n",
      "Epoch 2, Batch 140, Loss: 0.5559\n",
      "Epoch 2, Batch 160, Loss: 0.5652\n",
      "Epoch 2, Batch 180, Loss: 0.5363\n",
      "Epoch 2, Batch 200, Loss: 0.5200\n",
      "Epoch 2, Batch 220, Loss: 0.5482\n",
      "Epoch 2, Batch 240, Loss: 0.5272\n",
      "Epoch 2, Batch 260, Loss: 0.5470\n",
      "Epoch 2, Batch 280, Loss: 0.5490\n",
      "Epoch 2, Batch 300, Loss: 0.5384\n",
      "Epoch 2, Batch 320, Loss: 0.5316\n",
      "Epoch 2, Batch 340, Loss: 0.5426\n",
      "Epoch 2, Batch 360, Loss: 0.5112\n",
      "Epoch 2, Batch 380, Loss: 0.5165\n",
      "Epoch 2, Batch 400, Loss: 0.5574\n",
      "Epoch 2, Batch 420, Loss: 0.5427\n",
      "Epoch 2, Batch 440, Loss: 0.5360\n",
      "Epoch 2, Batch 460, Loss: 0.5299\n",
      "Epoch 2, Batch 480, Loss: 0.5507\n",
      "Epoch 2, Batch 500, Loss: 0.5349\n",
      "Epoch 2, Batch 520, Loss: 0.5297\n",
      "Epoch 2, Batch 540, Loss: 0.5317\n",
      "Epoch 2, Batch 560, Loss: 0.5613\n",
      "Epoch 2, Batch 580, Loss: 0.5391\n",
      "Epoch 2, Batch 600, Loss: 0.5587\n",
      "Epoch 2, Batch 620, Loss: 0.5701\n",
      "Epoch 2, Batch 640, Loss: 0.5317\n",
      "Epoch 2, Batch 660, Loss: 0.5422\n",
      "Epoch 2, Batch 680, Loss: 0.5165\n",
      "Epoch 2, Batch 700, Loss: 0.5578\n",
      "Epoch 2, Batch 720, Loss: 0.5488\n",
      "Epoch 2, Batch 740, Loss: 0.5189\n",
      "Epoch 2, Batch 760, Loss: 0.5025\n",
      "Epoch 2, Batch 780, Loss: 0.5210\n",
      "Train Accuracy: 0.639, Loss: 0.532\n",
      "Test Accuracy: 0.660, Loss: 0.527\n",
      "Epoch 3, Batch 0, Loss: 0.5417\n",
      "Epoch 3, Batch 20, Loss: 0.5156\n",
      "Epoch 3, Batch 40, Loss: 0.5311\n",
      "Epoch 3, Batch 60, Loss: 0.5148\n",
      "Epoch 3, Batch 80, Loss: 0.5347\n",
      "Epoch 3, Batch 100, Loss: 0.5428\n",
      "Epoch 3, Batch 120, Loss: 0.5341\n",
      "Epoch 3, Batch 140, Loss: 0.5144\n",
      "Epoch 3, Batch 160, Loss: 0.5307\n",
      "Epoch 3, Batch 180, Loss: 0.5439\n",
      "Epoch 3, Batch 200, Loss: 0.5332\n",
      "Epoch 3, Batch 220, Loss: 0.5133\n",
      "Epoch 3, Batch 240, Loss: 0.5310\n",
      "Epoch 3, Batch 260, Loss: 0.5396\n",
      "Epoch 3, Batch 280, Loss: 0.5167\n",
      "Epoch 3, Batch 300, Loss: 0.5281\n",
      "Epoch 3, Batch 320, Loss: 0.5503\n",
      "Epoch 3, Batch 340, Loss: 0.5547\n",
      "Epoch 3, Batch 360, Loss: 0.5394\n",
      "Epoch 3, Batch 380, Loss: 0.5360\n",
      "Epoch 3, Batch 400, Loss: 0.5502\n",
      "Epoch 3, Batch 420, Loss: 0.5226\n",
      "Epoch 3, Batch 440, Loss: 0.5460\n",
      "Epoch 3, Batch 460, Loss: 0.5080\n",
      "Epoch 3, Batch 480, Loss: 0.5353\n",
      "Epoch 3, Batch 500, Loss: 0.5289\n",
      "Epoch 3, Batch 520, Loss: 0.5181\n",
      "Epoch 3, Batch 540, Loss: 0.5394\n",
      "Epoch 3, Batch 560, Loss: 0.5252\n",
      "Epoch 3, Batch 580, Loss: 0.5112\n",
      "Epoch 3, Batch 600, Loss: 0.5348\n",
      "Epoch 3, Batch 620, Loss: 0.5620\n",
      "Epoch 3, Batch 640, Loss: 0.5309\n",
      "Epoch 3, Batch 660, Loss: 0.5520\n",
      "Epoch 3, Batch 680, Loss: 0.5194\n",
      "Epoch 3, Batch 700, Loss: 0.5126\n",
      "Epoch 3, Batch 720, Loss: 0.5331\n",
      "Epoch 3, Batch 740, Loss: 0.5067\n",
      "Epoch 3, Batch 760, Loss: 0.5366\n",
      "Epoch 3, Batch 780, Loss: 0.5328\n",
      "Train Accuracy: 0.640, Loss: 0.532\n",
      "Test Accuracy: 0.662, Loss: 0.527\n",
      "Epoch 4, Batch 0, Loss: 0.5222\n",
      "Epoch 4, Batch 20, Loss: 0.5548\n",
      "Epoch 4, Batch 40, Loss: 0.5148\n",
      "Epoch 4, Batch 60, Loss: 0.5190\n",
      "Epoch 4, Batch 80, Loss: 0.5211\n",
      "Epoch 4, Batch 100, Loss: 0.5315\n",
      "Epoch 4, Batch 120, Loss: 0.5461\n",
      "Epoch 4, Batch 140, Loss: 0.5550\n",
      "Epoch 4, Batch 160, Loss: 0.5399\n",
      "Epoch 4, Batch 180, Loss: 0.5123\n",
      "Epoch 4, Batch 200, Loss: 0.5406\n",
      "Epoch 4, Batch 220, Loss: 0.5334\n",
      "Epoch 4, Batch 240, Loss: 0.5221\n",
      "Epoch 4, Batch 260, Loss: 0.5056\n",
      "Epoch 4, Batch 280, Loss: 0.5510\n",
      "Epoch 4, Batch 300, Loss: 0.5187\n",
      "Epoch 4, Batch 320, Loss: 0.5517\n",
      "Epoch 4, Batch 340, Loss: 0.5239\n",
      "Epoch 4, Batch 360, Loss: 0.5464\n",
      "Epoch 4, Batch 380, Loss: 0.5273\n",
      "Epoch 4, Batch 400, Loss: 0.5656\n",
      "Epoch 4, Batch 420, Loss: 0.5230\n",
      "Epoch 4, Batch 440, Loss: 0.5194\n",
      "Epoch 4, Batch 460, Loss: 0.5471\n",
      "Epoch 4, Batch 480, Loss: 0.5373\n",
      "Epoch 4, Batch 500, Loss: 0.5278\n",
      "Epoch 4, Batch 520, Loss: 0.5288\n",
      "Epoch 4, Batch 540, Loss: 0.5203\n",
      "Epoch 4, Batch 560, Loss: 0.5219\n",
      "Epoch 4, Batch 580, Loss: 0.5553\n",
      "Epoch 4, Batch 600, Loss: 0.5214\n",
      "Epoch 4, Batch 620, Loss: 0.5248\n",
      "Epoch 4, Batch 640, Loss: 0.5356\n",
      "Epoch 4, Batch 660, Loss: 0.5185\n",
      "Epoch 4, Batch 680, Loss: 0.5165\n",
      "Epoch 4, Batch 700, Loss: 0.5466\n",
      "Epoch 4, Batch 720, Loss: 0.5230\n",
      "Epoch 4, Batch 740, Loss: 0.5443\n",
      "Epoch 4, Batch 760, Loss: 0.4936\n",
      "Epoch 4, Batch 780, Loss: 0.5180\n",
      "Train Accuracy: 0.628, Loss: 0.532\n",
      "Test Accuracy: 0.647, Loss: 0.527\n",
      "Epoch 5, Batch 0, Loss: 0.5219\n",
      "Epoch 5, Batch 20, Loss: 0.5256\n",
      "Epoch 5, Batch 40, Loss: 0.5326\n",
      "Epoch 5, Batch 60, Loss: 0.5137\n",
      "Epoch 5, Batch 80, Loss: 0.5104\n",
      "Epoch 5, Batch 100, Loss: 0.5403\n",
      "Epoch 5, Batch 120, Loss: 0.5212\n",
      "Epoch 5, Batch 140, Loss: 0.5227\n",
      "Epoch 5, Batch 160, Loss: 0.5244\n",
      "Epoch 5, Batch 180, Loss: 0.5554\n",
      "Epoch 5, Batch 200, Loss: 0.5399\n",
      "Epoch 5, Batch 220, Loss: 0.5203\n",
      "Epoch 5, Batch 240, Loss: 0.5413\n",
      "Epoch 5, Batch 260, Loss: 0.5090\n",
      "Epoch 5, Batch 280, Loss: 0.5190\n",
      "Epoch 5, Batch 300, Loss: 0.5251\n",
      "Epoch 5, Batch 320, Loss: 0.5441\n",
      "Epoch 5, Batch 340, Loss: 0.5047\n",
      "Epoch 5, Batch 360, Loss: 0.5112\n",
      "Epoch 5, Batch 380, Loss: 0.5028\n",
      "Epoch 5, Batch 400, Loss: 0.5260\n",
      "Epoch 5, Batch 420, Loss: 0.5289\n",
      "Epoch 5, Batch 440, Loss: 0.5425\n",
      "Epoch 5, Batch 460, Loss: 0.5336\n",
      "Epoch 5, Batch 480, Loss: 0.5155\n",
      "Epoch 5, Batch 500, Loss: 0.5248\n",
      "Epoch 5, Batch 520, Loss: 0.5041\n",
      "Epoch 5, Batch 540, Loss: 0.5269\n",
      "Epoch 5, Batch 560, Loss: 0.5427\n",
      "Epoch 5, Batch 580, Loss: 0.5300\n",
      "Epoch 5, Batch 600, Loss: 0.5493\n",
      "Epoch 5, Batch 620, Loss: 0.5438\n",
      "Epoch 5, Batch 640, Loss: 0.5654\n",
      "Epoch 5, Batch 660, Loss: 0.5486\n",
      "Epoch 5, Batch 680, Loss: 0.5641\n",
      "Epoch 5, Batch 700, Loss: 0.5383\n",
      "Epoch 5, Batch 720, Loss: 0.5384\n",
      "Epoch 5, Batch 740, Loss: 0.5306\n",
      "Epoch 5, Batch 760, Loss: 0.5026\n",
      "Epoch 5, Batch 780, Loss: 0.5261\n",
      "Train Accuracy: 0.621, Loss: 0.532\n",
      "Test Accuracy: 0.642, Loss: 0.527\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ✅ Load and continue training\n",
    "\n",
    "# 1. 모델 객체 생성\n",
    "model = TorchQNN().to(device)\n",
    "\n",
    "# 2. 저장된 파라미터 불러오기\n",
    "model.load_state_dict(torch.load(\"torchqnn_mnist.pth\"))\n",
    "model.train()  # 학습 모드로 전환\n",
    "\n",
    "# 3. 옵티마이저 & Loss 재설정\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 4. 이어서 학습\n",
    "train(model, train_dataset, test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa9277a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qiskit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
